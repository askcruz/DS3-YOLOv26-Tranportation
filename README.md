# DS3-YOLOv26-Tranportation
Group Project by:
* Cruz, Audrize
* Morales, Rayleen

---
# I. Introduction
Our primary objective in this assessment is to deploy and evaluate the YOLOv26 who introduced NMS-free inference and optimized computational pathways suitable for real-world deployment architecture (Ultralytics, 2026). Rather than focusing solely on training accuracy, we emphasized model diagnostics and interpretability using the vehicle traffic dataset prepared using different balancing strategies (Chien, 2025; Crasto, 2024). 

To achieve this objective, multiple model configurations are trained and compared using different hyperparameter settings, contributing to evaluation studies that reveal how modern YOLO model variants optimize precisionâ€“recall trade-offs (Sapkota 2026; Ultralytics, 2026a). Model performance is then evaluated through an analysis of the confusion matrix to identify class-level strengths and weaknesses and the extraction of statistical performance metrics, including mAP50, precision, recall, and F1 score.

# II. Three Model Setup
To evaluate how the prescribed different training configuration would affect object detection performance, we trained three YOLOv26n models using the same balanced dataset and preprocessing pipeline. 

All models were initialized using the YOLOv26 nano pretrained weights and were trained with a fixed input image size of 640 Ã— 640. By keeping the dataset, architecture, and image resolution constant across all runs, we ensured that any differences in performance could be attributed to changes in training hyperparameters rather than variations in data preparation or input scaling. Below is a table detailing the configurations of the three models.


# III. Model Evaluation
## Fianl Epoch Results
To evaluate the performance of each training configuration, we examined the results obtained at the final epoch of all three models. This part of the discussion focuses on detection accuracy, class prediction behavior, and convergence patterns based on precision, recall, mAP values, and loss metrics.
	For Model 1 (AdamW), it achieved a precision of 0.46643 and a recall of 0.64309, resulting in an mAP50 of 0.53918 and an mAP50â€“95 of 0.35177. The relatively high recall indicates that the model was able to detect a large proportion of ground-truth objects; however, the lower precision suggests that this detection behavior was accompanied by a higher number of false positives. The training losses at the final epoch were 1.16941 for box loss, 1.36557 for classification loss, and 0.01475 for DFL loss. Validation losses followed a similar pattern, with val_box_loss = 1.30535, val_cls_loss = 1.48241, and val_dfl_loss = 0.01712. The similarity between training and validation losses indicates that the model converged without severe overfitting, although the elevated classification loss suggests difficulty in distinguishing between classes with high confidence.
	On the other hand, Model 2 (SGD) recorded a precision of 0.49003 and a recall of 0.50059, producing an mAP50 of 0.45513 and an mAP50â€“95 of 0.30305. Compared to Model 1, this configuration exhibited a more balanced precisionâ€“recall relationship, but overall detection accuracy remained lower. The training losses at the final epoch were 1.05247 (box loss), 3.22357 (classification loss), and 0.01280 (DFL loss). Notably, the classification loss was substantially higher than that of the other models, indicating persistent difficulty in class separation. This behavior was also reflected in the validation classification loss (1.74357), which remained elevated.
	Finally, Model 3 (Auto), achieved the strongest results at the final epoch (epoch 40), with a precision of 0.59157, a recall of 0.74413, and an mAP50 of 0.68527. The mAP50â€“95 value of 0.51433 further indicates improved localization performance across stricter IoU thresholds. Training losses were the lowest among all models, with 0.83008 (box loss), 0.98356 (classification loss), and 0.00994 (DFL loss). Validation losses were also consistently lower, particularly val_box_loss = 0.96319 and val_cls_loss = 1.03526, suggesting better generalization and reduced classification error.
Across all three models, Model 3 clearly outperformed Models 1 and 2 in terms of both detection accuracy and stability. Model 1 favored recall at the expense of precision, while Model 2 exhibited conservative detection behavior with limited overall accuracy. In contrast, Model 3 demonstrated the most favorable trade-off between detecting true objects and minimizing false predictions. Based on the final epoch results, Model 3 shows the strongest potential for real-world deployment among the tested configurations, as it achieved the highest mAP values while maintaining balanced precision and recall.

### A screenshot or text output of the final epoch results.
<img width="1139" height="313" alt="FinalEpochResults" src="https://github.com/user-attachments/assets/48e3cf27-b44b-406d-a53f-0f585560bc54" />

### The plotted confusion matrix image is displayed within the notebook.

# Confusion Matrixc Analysis
## Model 1 - AdamW
From the normalized confusion matrix, we observe that the AdamW model demonstrates moderate true positive rates across most classes, but with a consistent tendency to confuse foreground objects with the background class. For example, in the Bike class, the model correctly classified 51% of bike instances. However, 49% of true bike samples were predicted as background. This indicates that while the model learned visual features associated with bikes, nearly half of the instances were not confidently detected as foreground objects. Car detection exhibited a notably low true positive rate of 23%, with a dominant 77% of car instances misclassified as background. This suggests that despite cars being visually prominent, the model struggled to distinguish them from the surrounding environment, likely due to occlusions, scale variations, or background dominance in the scene. Motorcycle instances were correctly identified 41% of the time, while 59% were classified as background, while the Others class achieved a true positive rate of 58%, with 42% of samples predicted as background. Tricycles were correctly classified 58% of the time, but 42% were predicted as background and Trucks achieved the highest true positive rate among all foreground classes at 62%, with 37% misclassified as background.  
Analysis of the background row in the raw confusion matrix reveals a substantial number of false positives, where background regions were incorrectly classified as vehicles. Background was misclassified as Bike (40 instances), Motorcycle (39 instances), and Car (20 instances), and smaller but still present false positives occurred for Truck (11), Tricycle (7), and Others (2).
The confusion matrix reveals that the AdamW-based model exhibits a foregroundâ€“background imbalance, where a significant proportion of true vehicle instances are absorbed into the background class, while background regions are occasionally misidentified as vehicles. Although the model shows reasonable detection capability for larger and more visually distinct classes such as trucks, its performance degrades for smaller or more ambiguous vehicles like cars, motorcycles, and bikes. These findings suggest that while AdamW enabled stable convergence, the configuration may require longer training, refined learning rate scheduling, or stronger class-specific augmentation to improve foreground confidence and reduce background confusion. The confusion matrix therefore provides critical insight into why Model 1 achieved moderate mAP performance but struggled with precision at the class level.

<img width="673" height="490" alt="model1_adamw-raw" src="https://github.com/user-attachments/assets/b31452d6-46f8-4f6a-a373-0649897850c1" />
<img width="678" height="490" alt="model1_adamw-normalized" src="https://github.com/user-attachments/assets/ce55aaca-52b9-402a-8a38-e0afe2b6518f" />

## Model 2 - SGD Optimizer
To evaluate the classification behavior of Model 2 trained using the SGD optimizer, we analyzed both the raw count and normalized confusion matrices. Based on the normalized confusion matrix, Model 2 demonstrates slightly improved class separation for some vehicle types compared to Model 1, but still exhibits a persistent tendency to misclassify objects as background. 
For the Bike class, the model correctly classified 50% of bike instances, while the remaining 50% were predicted as background. This balanced split indicates that the model learned basic bike-related features, but detection confidence remained inconsistent. Meanwhile, Car detection remained weak, with only 26% of car instances correctly classified. A substantial 74% of car samples were misclassified as background. Despite cars being common and visually distinct, the model struggled to consistently identify them as foreground objects. Motorcycle performance improved relative to Model 1, achieving a true positive rate of 54%, while 46% were classified as background, while the Others class achieved a relatively strong true positive rate of 66%, with 34% misclassified as background. This indicates that the model was better able to generalize across heterogeneous object types in this category. Tricycles were correctly identified 64% of the time, while 36% were classified as background, and Trucks achieved a 63% true positive rate, with 37% misclassified as background.
The raw confusion matrix reveals a considerable number of false positives originating from the background class. This means that background regions were misclassified as Bike (49 instances) and Motorcycle (45 instances) most frequently and additional false positives occurred for Truck (34), Car (19), and Tricycle (19). This pattern indicates that the model often interpreted background features, such as road textures, shadows, or partial vehicle edges, as valid vehicle objects. While this behavior increases sensitivity, it also contributes to reduced precision and reinforces the trade-off observed in the final epoch metrics. In conclusion, model 2 exhibits more balanced class recognition than Model 1 for certain categories, particularly motorcycles, tricycles, and trucks. However, the model continues to struggle with foregroundâ€“background separation, especially for cars and bikes.

<img width="673" height="490" alt="model2_sgd-raw" src="https://github.com/user-attachments/assets/4e30bb3a-26c1-41c9-87a5-c1f2224da266" />
<img width="678" height="490" alt="model2_sgd-normalized" src="https://github.com/user-attachments/assets/b3b5d0fb-b2bf-4825-988b-f54c82c54d0e" />

## Model 3 - Auto Optimizer
Finally, the normalized and raw count confusion matrices of Model 3, trained using the Auto optimizer configuration, demonstrates clearer class separation and improved foreground detection, which is consistent with its superior performance in the final epoch metrics. In the normalized confusion matrix, the Bike class correctly classified 57% of bike instances, with 43% misclassified as background. While background confusion is still present, this represents an improvement over both Model 1 and Model 2, suggesting better sensitivity to smaller vehicle features. Car detection remained challenging, with a true positive rate of 25% and 75% of instances predicted as background. Despite this limitation, the raw count matrix shows that the model successfully detected 28 car instances, indicating incremental improvement compared to earlier models, though cars remain the most difficult class to separate from the background. On the other hand, Motorcycle performance improved notably, with 50% of instances correctly classified and 50% misclassified as background, and the Others class achieved a 63% true positive rate, with 37% predicted as background. Notably, Tricycles were correctly identified 69% of the time, the highest among all models, and rucks achieved the strongest performance overall, with a 75% true positive rate and only 25% background misclassification. The raw count matrix confirms 54 correctly detected truck instances, telling up the modelâ€™s reliability for larger and more visually distinct vehicles.
	Analysis of the background row in the raw confusion matrix shows a reduction in false positives compared to the previous models with background most frequently misclassified as Motorcycle (28) and Bike (23) and with fewer false positives occurred for Truck (17), Tricycle (7), Car (6), and Others (1). Although background confusion was not fully eliminated, the reduced magnitude of false positives indicates improved discrimination between foreground objects and background elements. This aligns with the higher precision observed in Model 3â€™s final epoch results, reaffirming that Model 3 exhibits the most consistent and reliable classification behavior among the three models

<img width="673" height="490" alt="model3_auto-raw" src="https://github.com/user-attachments/assets/49399d97-79c5-4b2c-a8a2-e5b950157d4e" />
<img width="678" height="490" alt="model3_auto-normalized" src="https://github.com/user-attachments/assets/ceb98acd-2170-4ea3-b3bb-ceb0477d51d9" />

## A formatted table (using Pandas) showing the 3 models' hyperparameter settings.
<img width="473" height="97" alt="HyperparameterTable" src="https://github.com/user-attachments/assets/ef30397f-d625-4dfa-94e0-926eb6962f21" />

## A formatted table (using pandas) showing the 3 modes' metrics results.
<img width="278" height="127" alt="PerformanceTable" src="https://github.com/user-attachments/assets/3f7bec8b-647b-4f4e-b4e6-78cedc1cbaff" />

After the confusion matrices, we analyzed the performance metrics summarized in the table, specifically mAP50, precision, recall, and F1 score. Model 1 achieved an mAP50 of 0.5388, with a precision of 0.4660 and a recall of 0.6440, resulting in an F1 score of 0.5407. This pattern aligns with the confusion matrix analysis, where Model 1 frequently misclassified background regions as foreground objects. As a result, Model 1 demonstrated moderate detection capability but lacked consistency in class confidence. 
	Following this, Model 2 recorded the lowest overall performance, with an mAP50 of 0.4559, a precision of 0.4869, and a recall of 0.5006, producing an F1 score of 0.4937. Compared to Model 1, Model 2 exhibited a more balanced precisionâ€“recall relationship, but this balance occurred at a lower performance level. The reduced mAP50 and F1 score indicate weaker localization and classification accuracy, which is consistent with the confusion matrix results showing persistent background confusion and limited improvement in car detection. While the SGD optimizer provided stable training behavior, it did not sufficiently enhance the modelâ€™s ability to learn discriminative features for this dataset.
	Finally, Model 3 outperformed both previous configurations across all metrics. It achieved the highest mAP50 of 0.6864, along with a precision of 0.6354, a recall of 0.6737, and an F1 score of 0.6540. The higher precision reflects improved foregroundâ€“background separation, while the elevated recall confirms that the model maintained strong sensitivity to vehicle instances. The resulting F1 score demonstrates that Model 3 achieved the most consistent and reliable detection performance among the three models.

# IV. Per-Class Metrics
Across all evaluated metrics, Model 3 (Auto) consistently outperformed Model 1 (AdamW) and Model 2 (SGD). It achieved the highest values for most classes in terms of precision, recall, F1 score, and AP50, indicating stronger class separation and more reliable localization. In contrast, Model 1 favored recall over precision, while Model 2 exhibited limited overall improvement. 
Model 1 consistently exhibited a recall-heavy detection pattern, particularly for larger or more visually distinct classes. For example, recall values were high for Truck (0.9296), Others (0.8571), and Tricycle (0.7679). However, these gains were offset by comparatively low precision values, such as 0.3966 for Truck and 0.4653 for Tricycle, indicating frequent false positives. This imbalance is further reflected in the F1 scores. While the model achieved moderate F1 values for Others (0.6336) and Tricycle (0.5794), performance dropped significantly for smaller or more ambiguous classes such as Motorcycle (0.2348) and Car (0.2592). Correspondingly, AP@50 values were lowest for these classes, particularly Motorcycle (0.1705), suggesting this is where the model struggled the most. Furthermore, the model also deeply struggled with the Car class with an F1 score of 0.2592 and AP@50 of 0.2566. Despite cars being common objects in traffic scenes, the model frequently misclassified them as background, as previously observed in the confusion matrix.
Model 2 demonstrated improved precision for several classes compared to Model 1, particularly for Bike (0.4414), Motorcycle (0.4340), and Truck (0.6042). However, this increase in precision was accompanied by reduced recall for certain categories, most notably Motorcycle (0.1346) and Bike (0.2698). Despite these trade-offs, Model 2 achieved stronger balance for mid-sized vehicle classes. The Tricycle class achieved an F1 score of 0.6249, while Others reached 0.7173, indicating more stable detection performance. AP@50 values followed a similar trend, with relatively consistent results across Others (0.6971) and Tricycle (0.6590).  This leaves Model 2 showing its poorest performance on the Motorcycle class as well, but for different reasons from Model 1. Although precision improved to 0.4340, recall dropped sharply to 0.1346, resulting in the lowest F1 score (0.2055) among all modelâ€“class combinations. This indicates that the model was overly conservative, failing to detect many true motorcycle instances. The Bike class was another area of difficulty for Model 2. With a recall of only 0.2698 and an AP@50 of 0.2292, the model frequently missed bike instances, even when precision was moderately higher than Model 1. Nevertheless, Model 2 struggled to generalize effectively across all classes. Its lower recall for smaller vehicles limited overall detection robustness.
Model 3 achieved the most balanced and consistent performance across all classes, outperforming the other models in nearly every metric. Precision values were substantially higher, particularly for Truck (0.7859), Tricycle (0.7250), and Bike (0.6773), indicating improved confidence in class predictions. At the same time, recall remained competitive or superior for most classes. Notably, Car recall increased to 0.7941, representing a significant improvement over both Model 1 and Model 2. This improvement translated into higher F1 scores across all categories, with particularly strong results for Others (0.7531), Tricycle (0.7458), and Truck (0.7247). AP@50 values further confirm Model 3â€™s effectiveness. It achieved the highest AP@50 scores for every class, including Truck (0.8452), Tricycle (0.7965), and Others (0.7719). Even traditionally challenging classes such as Motorcycle (0.4569) and Car (0.5365) showed substantial improvement, indicating better localization and reduced background interference. However, while Model 3 achieved the strongest overall performance, it still showed relative weakness in the Motorcycle class compared to other vehicle types. Although precision increased substantially to 0.6432, recall remained moderate at 0.3269, resulting in an F1 score of 0.4335. While this is a significant improvement over Models 1 and 2, motorcycle detection remained the least robust class within Model 3â€™s results.

<img width="311" height="767" alt="PerClassMetricsTable1" src="https://github.com/user-attachments/assets/5d3b08f1-5ba0-4c29-8572-90e0fe506238" />
<img width="353" height="421" alt="PerClassMetricsTable2" src="https://github.com/user-attachments/assets/658b96f6-25e6-4758-8292-f1cf33f85caf" />


# V. Conclusion
In this assessment, we successfully implemented and evaluated the YOLOv26 architecture with NMS-free detection using multiple training configurations to examine how optimizer choice and hyperparameter settings influence object detection performance. We found that Model 3 (Auto) consistently outperformed the other two models, achieving the highest mAP50, F1 score, and the most balanced precisionâ€“recall trade-off across all classes. Despite motorcycles remaining the most challenging class overall, Model 3 demonstrated improved robustness and generalization. These findings emphasize the importance of adaptive optimization strategies when deploying YOLO-based models for complex, real-world traffic detection tasks.

# VI. References
* Crasto, N. (2024). Class Imbalance in Object Detection: An Experimental Diagnosis and Study of Mitigation Strategies. https://doi.org/10.48550/arXiv.2403.07113
* Sapkota, R., Meng, Z., Churuvija, M., Du, X., Ma, Z., & Karkee, M. (2026). Comprehensive Performance Evaluation of YOLOv12, YOLO11, YOLOv10, YOLOv9 and YOLOv8 on Detecting and Counting Fruitlet in Complex Orchard Environments. Agriculture Communications, 100125. https://doi.org/10.1016/j.agrcom.2026.100125
* Ultralytics. (2026). Ultralytics YOLO26. Ultralytics.com. https://docs.ultralytics.com/models/yolo26/?utm_source=chatgpt.com#how-do-i-get-started-with-yolo26
* Ultralytics. (2026a, November 12). Models. Docs.ultralytics.com. https://docs.ultralytics.com/models/
* Vu, T. C., Tran, T. D., Nguyen, T. V., Nguyen, D. T., Dinh, L. Q., Nguyen, M. D., Nguyen, H. T., & Nguyen, M. T. (2025). Vehicle Detection, Tracking and Counting in Traffic Video Streams Based on the Combination of YOLOv9 and DeepSORT Algorithms. Journal of Future Artificial Intelligence and Technologies, 2(2), 255â€“268. https://doi.org/10.62411/faith.3048-3719-115


# ðŸ“Œ License & Attribution
This project is licensed under the MIT License.

This repository and its accompanying dataset are open for public use. You are free to reference, adapt, or build upon this work for academic or personal projects, provided that proper credit is given to the original authors.

Please cite as:

Cruz, A., & Morales, R. (2026). DS3-YOLOv26-Transportation [GitHub repository]. GitHub. https://github.com/askcruz/DS3-YOLOv26-Tranportation/
